layer {
  name: "data"
  type: "PatchData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  patch_sampler_param{
	   data_patch_shape{
		dim: 320
		dim: 320
		dim: 1
	   }
	   label_patch_shape{
		dim: 320
		dim: 320
		dim: 1
	   }
	   batch_size: 5
	   patches_per_data_batch: 7999999
	   #data_source_batch_size: 1

 }

label_select_param{
	balance: true
	num_labels: 2
	num_top_label_balance: 2
	reorder_label: false
	class_prob_mapping_file: "label_class_selection.prototxt"
}
  transform_nd_param{
    mirror: true
    padding: true
    pad_method: ZERO
  }
  data_provider_param{
    # Specify the data source.
    backend: HDF5
    batch_size: 16
    hdf5_file_shuffle: true
    data_source: "train_file.txt"
  }

}


layer {
  name: "data"
  type: "PatchData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  patch_sampler_param{
	   data_patch_shape{
		dim: 320
		dim: 320
		dim: 1
	   }
	   label_patch_shape{
		dim: 320
		dim: 320
		dim: 1
	   }
	   batch_size: 2
	   patches_per_data_batch: 6899999
	   #data_source_batch_size: 1
 }

 label_select_param{
			   balance: true
			   num_labels: 2
			   num_top_label_balance: 2
			   reorder_label: false
			   class_prob_mapping_file: "label_class_selection.prototxt"
		}
  transform_nd_param{
    mirror: false
    padding: true
    pad_method: ZERO
  }
  data_provider_param{
    # Specify the data source.
    backend: HDF5
    batch_size: 1
    hdf5_file_shuffle: true
    data_source:  "valid_file.txt"
  }

}


layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output: 32
    kernel_size: 3 kernel_size: 3  kernel_size: 1
    stride: 2 stride: 2 stride: 1
    pad:1 pad:1 pad:0
    weight_filler {  type: "gaussian" std: 0.01}
    bias_filler {type: "constant" value: 0}
  }
}


layer {
	bottom: "conv1_1"
	top: "conv1_1"
	name: "bn_conv1_1"
	type: "BatchNorm"
}

layer {
	bottom: "conv1_1"
	top: "conv1_1"
	name: "scale_conv1_1"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}

layer {
  name: "conv1_2"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_2"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output: 64
    kernel_size: 3 kernel_size: 3  kernel_size: 1
    stride: 1
    pad:1 pad:1 pad:0
    weight_filler {  type: "gaussian" std: 0.01}
    bias_filler {type: "constant" value: 0}
  }
}

layer {
	bottom: "conv1_2" 	top: "conv1_2" 	name: "bn_conv1_2"	type: "BatchNorm"
#	# batch_norm_param {use_global_stats: true}
}

layer {
	bottom: "conv1_2" 	top: "conv1_2" 	name: "scale_conv1_2"	type: "Scale"
	scale_param {	bias_term: true}
}
layer {
  name: "relu1_2" type: "ReLU" bottom: "conv1_2" top: "conv1_2"
}


layer {
  name: "reshape"
  type: "Reshape"
  bottom: "conv1_2"
  top: "conv1_2"
  reshape_param { shape: {dim: 0 dim: 0 dim: 0 dim: 0 } }
}

layer {
  name: "reshape"
  type: "Reshape"
  bottom: "label"
  top: "label"
  reshape_param { shape: {dim: 0 dim: 0 dim: 0 dim: 0 } }
}


# layer {
  # name: "pool1"
  # type: "Pooling"
  # bottom: "conv1_2"
  # top: "pool1"
  # top: "pool1_mask"
  # pooling_param {
    # pool: MAX
  # kernel_size: 2
  # stride: 2
  # }
# } 


layer {
  name: "conv2_1b"
  type: "Convolution"
  bottom: "conv1_2"
  top: "conv2_1b"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:64
    kernel_size: 1
    stride: 1 pad:0
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "conv2_1b" 	top: "conv2_1b" 	name: "bn_conv2_1b"	type: "BatchNorm"
#	# batch_norm_param {use_global_stats: true}
}

layer {
	bottom: "conv2_1b" 	top: "conv2_1b" 	name: "scale_conv2_1b"	type: "Scale"
	scale_param {	bias_term: true}
}
 layer {
name: "relu2_1b" type: "ReLU" bottom: "conv2_1b" top: "conv2_1b"
 }
 
 
 
layer {
  name: "conv2_1b_3x3"
  type: "Convolution"
  bottom: "conv2_1b"
  top: "conv2_1b_3x3"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output: 96
    kernel_size: 3
    stride: 1 pad:1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "conv2_1b_3x3" 	top: "conv2_1b_3x3" 	name: "bn_conv2_1b_3x3"	type: "BatchNorm"
#	# batch_norm_param {use_global_stats: true}
}

layer {
	bottom: "conv2_1b_3x3" 	top: "conv2_1b_3x3" 	name: "scale_conv2_1b_3x3"	type: "Scale"
	scale_param {	bias_term: true}
}
# layer {
  # name: "relu2_1b_3x3" type: "ReLU" bottom: "conv2_1b_3x3" top: "conv2_1b_3x3"
# }
 

 
layer {
  name: "conv2_1x1"
  type: "Convolution"
  bottom: "conv1_2"
  top: "conv2_1x1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:64
    kernel_size: 1
    stride: 1 pad:0
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "conv2_1x1" 	top: "conv2_1x1" 	name: "bn_conv2_1x1"	type: "BatchNorm"
}

layer {
	bottom: "conv2_1x1" 	top: "conv2_1x1" 	name: "scale_conv2_1x1"	type: "Scale"
	scale_param {	bias_term: true}
}
 layer {
name: "relu2_1x1" type: "ReLU" bottom: "conv2_1x1" top: "conv2_1x1"
 }
 
 

layer {
  name: "conv2_1x7"
  type: "Convolution"
  bottom: "conv2_1x1"
  top: "conv2_1x7"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:64
    kernel_w: 1 kernel_h: 7
    stride: 1 pad_w:0 pad_h:3
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "conv2_1x7" 	top: "conv2_1x7" 	name: "bn_conv2_1x7"	type: "BatchNorm"
#	# batch_norm_param {use_global_stats: true}
}

layer {
	bottom: "conv2_1x7" 	top: "conv2_1x7" 	name: "scale_conv2_1x7"	type: "Scale"
	scale_param {	bias_term: true}
}
 layer {
name: "relu2_1x7" type: "ReLU" bottom: "conv2_1x7" top: "conv2_1x7"
 }
 
 
layer {
  name: "conv2_7x1"
  type: "Convolution"
  bottom: "conv2_1x7"
  top: "conv2_7x1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:64
    kernel_w: 7 kernel_h: 1
    stride: 1 pad_w:3 pad_h:0
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "conv2_7x1" 	top: "conv2_7x1" 	name: "bn_conv2_7x1"	type: "BatchNorm"
#	# batch_norm_param {use_global_stats: true}
}

layer {
	bottom: "conv2_7x1" 	top: "conv2_7x1" 	name: "scale_conv2_7x1"	type: "Scale"
	scale_param {	bias_term: true}
}
 layer {
name: "relu2_7x1" type: "ReLU" bottom: "conv2_7x1" top: "conv2_7x1"
 }
 


layer {
  name: "conv2_3x3"
  type: "Convolution"
  bottom: "conv2_7x1"
  top: "conv2_3x3"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:96
    kernel_size: 3
    stride: 1 pad:1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "conv2_3x3" 	top: "conv2_3x3" 	name: "bn_conv2_3x3"	type: "BatchNorm"
#	# batch_norm_param {use_global_stats: true}
}

layer {
	bottom: "conv2_3x3" 	top: "conv2_3x3" 	name: "scale_conv2_3x3"	type: "Scale"
	scale_param {	bias_term: true}
}
 layer {
 name: "relu2_3x3" type: "ReLU" bottom: "conv2_3x3" top: "conv2_3x3"
 }

 
 layer{
 name: "concat_stem_1"
 type: "Concat"
 bottom:"conv2_1b_3x3"
 bottom: "conv2_3x3"
 top: "concat_stem_1"
 }
 
 
 layer {
  name: "stem_concat_conv_3x3"
  type: "Convolution"
  bottom: "concat_stem_1"
  top: "stem_concat_conv_3x3"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:192
    kernel_size: 3
    stride: 2 pad:1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "stem_concat_conv_3x3" 	top: "stem_concat_conv_3x3" 	name: "bn_stem_concat_conv_3x3"	type: "BatchNorm"
}

layer {
	bottom: "stem_concat_conv_3x3" 	top: "stem_concat_conv_3x3" 	name: "scale_stem_concat_conv_3x3"	type: "Scale"
	scale_param {	bias_term: true}
}
 layer {
name: "relu_stem_concat_conv_3x3" type: "ReLU" bottom: "stem_concat_conv_3x3" top: "stem_concat_conv_3x3"
 }
 
 layer {
  name: "pool_stem_concat"
  type: "Pooling"
  bottom: "concat_stem_1"
  top: "pool_stem_concat"
  #top: "pool2_mask"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}

 layer{
 name: "concat_stem_2"
 type: "Concat"
 bottom:"pool_stem_concat"
 bottom: "stem_concat_conv_3x3"
 top: "concat_stem_2"
 }
 


#---------------------------inception_Res_A--------------------------------------
layer {
  name: "conv3_1b"
  type: "Convolution"
  bottom: "concat_stem_2"
  top: "conv3_1b"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1 pad:0
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "conv3_1b" 	top: "conv3_1b" 	name: "bn_conv3_1b"	type: "BatchNorm"
	# batch_norm_param {use_global_stats: true}
}

layer {
	bottom: "conv3_1b" 	top: "conv3_1b" 	name: "scale_conv3_1b"	type: "Scale"
	scale_param {	bias_term: true}
}
layer {
   name: "relu3_1b" type: "ReLU" bottom: "conv3_1b" top: "conv3_1b"
  }
#-------a_1x1----
layer {
  name: "ira_A_1_conv1x1"
  type: "Convolution"
  bottom: "conv3_1b"
  top: "ira_A_1_conv1x1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:32
    kernel_size: 1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "ira_A_1_conv1x1" 	top: "ira_A_1_conv1x1" 	name: "bn_ira_A_1_conv1x1"	type: "BatchNorm"
}

layer {
	bottom: "ira_A_1_conv1x1" 	top: "ira_A_1_conv1x1" 	name: "scale_ira_A_1_conv1x1"	type: "Scale"
	scale_param {	bias_term: true}
}
 layer {
 name: "relu_ira_A_1_conv1x1" type: "ReLU" bottom: "ira_A_1_conv1x1" top: "ira_A_1_conv1x1"
 }

 #-------a2----------
layer {
  name: "ira_A_2_conv1x1"
  type: "Convolution"
  bottom: "conv3_1b"
  top: "ira_A_2_conv1x1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:32
    kernel_size: 1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "ira_A_2_conv1x1" 	top: "ira_A_2_conv1x1" 	name: "bn_ira_A_2_conv1x1"	type: "BatchNorm"
}

layer {
	bottom: "ira_A_2_conv1x1" 	top: "ira_A_2_conv1x1" 	name: "scale_ira_A_2_conv1x1"	type: "Scale"
	scale_param {	bias_term: true}
}
 layer {
 name: "relu_ira_A_2_conv1x1" type: "ReLU" bottom: "ira_A_2_conv1x1" top: "ira_A_2_conv1x1"
 }
 
 
 layer {
  name: "ira_A_2_conv3x3"
  type: "Convolution"
  bottom: "ira_A_2_conv1x1"
  top: "ira_A_2_conv3x3"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:32
    kernel_size: 3
	pad:1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "ira_A_2_conv3x3" 	top: "ira_A_2_conv3x3" 	name: "bn_ira_A_2_conv3x3"	type: "BatchNorm"
}

layer {
	bottom: "ira_A_2_conv3x3" 	top: "ira_A_2_conv3x3" 	name: "scale_ira_A_2_conv3x3"	type: "Scale"
	scale_param {	bias_term: true}
}
 layer {
 name: "relu_ira_A_2_conv3x3" type: "ReLU" bottom: "ira_A_2_conv3x3" top: "ira_A_2_conv3x3"
 }
 
 
 
 #-----------a3-------------------------------
 layer {
  name: "ira_A_3_conv1x1"
  type: "Convolution"
  bottom: "conv3_1b"
  top: "ira_A_3_conv1x1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:32
    kernel_size: 1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "ira_A_3_conv1x1" 	top: "ira_A_3_conv1x1" 	name: "bn_ira_A_3_conv1x1"	type: "BatchNorm"
}

layer {
	bottom: "ira_A_3_conv1x1" 	top: "ira_A_3_conv1x1" 	name: "scale_ira_A_3_conv1x1"	type: "Scale"
	scale_param {	bias_term: true}
}
 layer {
 name: "relu_ira_A_3_conv1x1" type: "ReLU" bottom: "ira_A_3_conv1x1" top: "ira_A_3_conv1x1"
 }
 
 
 
 
 layer {
  name: "ira_A_3_conv3x3_1"
  type: "Convolution"
  bottom: "ira_A_3_conv1x1"
  top: "ira_A_3_conv3x3_1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:48
    kernel_size: 3
	pad: 1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "ira_A_3_conv3x3_1" 	top: "ira_A_3_conv3x3_1" 	name: "bn_ira_A_3_conv3x3_1"	type: "BatchNorm"
}

layer {
	bottom: "ira_A_3_conv3x3_1" 	top: "ira_A_3_conv3x3_1" 	name: "scale_ira_A_3_conv3x3_1"	type: "Scale"
	scale_param {	bias_term: true}
}
 layer {
 name: "relu_ira_A_3_conv3x3_1" type: "ReLU" bottom: "ira_A_3_conv3x3_1" top: "ira_A_3_conv3x3_1"
 }
 
 
 
  
 layer {
  name: "ira_A_3_conv3x3_2"
  type: "Convolution"
  bottom: "ira_A_3_conv3x3_1"
  top: "ira_A_3_conv3x3_2"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:64
    kernel_size: 3
	pad: 1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "ira_A_3_conv3x3_2" 	top: "ira_A_3_conv3x3_2" 	name: "bn_ira_A_3_conv3x3_2"	type: "BatchNorm"
}

layer {
	bottom: "ira_A_3_conv3x3_2" 	top: "ira_A_3_conv3x3_2" 	name: "scale_ira_A_3_conv3x3_2"	type: "Scale"
	scale_param {	bias_term: true}
}
 layer {
 name: "relu_ira_A_3_conv3x3_2" type: "ReLU" bottom: "ira_A_3_conv3x3_2" top: "ira_A_3_conv3x3_2"
 }
 
 
 
 
 layer{
 name: "ira_A_concat"
 type: "Concat"
 bottom:"ira_A_1_conv1x1"
 bottom: "ira_A_2_conv3x3"
 bottom: "ira_A_3_conv3x3_2"
 top: "ira_A_concat"
 }
 
  
 layer {
  name: "ira_A_concat_top_conv_1x1"
  type: "Convolution"
  bottom: "ira_A_concat"
  top: "ira_A_concat_top_conv_1x1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:384
    kernel_size: 1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "ira_A_concat_top_conv_1x1" 	top: "ira_A_concat_top_conv_1x1" 	name: "bn_ra_A_concat_top_conv_1x1"	type: "BatchNorm"
}

layer {
	bottom: "ira_A_concat_top_conv_1x1" 	top: "ira_A_concat_top_conv_1x1" 	name: "scale_ra_A_concat_top_conv_1x1"	type: "Scale"
	scale_param {	bias_term: true}
}

layer{
     name: "conv3_sum"
     type:  "Eltwise"
	 bottom: "conv3_1b"
     bottom:"ira_A_concat_top_conv_1x1"
     top:"conv3_sum"
     eltwise_param {operation: SUM coeff: 1 coeff:0.1}
}


# layer {
 # bottom: "conv3_sum" 	top: "conv3_sum" 	name: "bn_conv3_sum"	type: "BatchNorm"
 # # batch_norm_param {use_global_stats: true}
# }

# layer {
 # bottom: "conv3_sum" 	top: "conv3_sum" 	name: "scale_conv3_sum"	type: "Scale"
 # scale_param {	bias_term: true}
# }
layer {name: "relu3_sum" type: "ReLU" bottom: "conv3_sum" top: "conv3_sum"}

#----------------------

##=================================REDUCTION_A========================
#-----reduction A Pooling a--------------
 layer {
  name: "ira_v4_reduction_A/pool"
  type: "Pooling"
  bottom: "conv3_sum"
  top: "ira_v4_reduction_A/pool"
  #top: "pool2_mask"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
#-----reduction A conv1x1_b--------------
 layer {
  name: "ira_v4_reduction_A/conv3x3_reduction_b"
  type: "Convolution"
  bottom: "conv3_sum"
  top: "ira_v4_reduction_A/conv3x3_reduction_b"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:384
    kernel_size: 3
	stride:2
	pad:1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "ira_v4_reduction_A/conv3x3_reduction_b" 	top: "ira_v4_reduction_A/conv3x3_reduction_b" 	name: "bn_ira_v4_reduction_A/conv3x3_reduction_b"	type: "BatchNorm"
}

layer {
	bottom: "ira_v4_reduction_A/conv3x3_reduction_b" 	top: "ira_v4_reduction_A/conv3x3_reduction_b" 	name: "scale_ira_v4_reduction_A/conv3x3_reduction_b"	type: "Scale"
	scale_param {	bias_term: true}
}
 layer {
 name: "relu_ira_v4_reduction_A/conv3x3_reduction_b" type: "ReLU" bottom: "ira_v4_reduction_A/conv3x3_reduction_b" top: "ira_v4_reduction_A/conv3x3_reduction_b"
 }

#-----reduction A conv_recdution _c --------------
 layer {
  name: "ira_v4_reduction_A/conv1x1_c"
  type: "Convolution"
  bottom: "conv3_sum"
  top: "ira_v4_reduction_A/conv1x1_c"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:256
    kernel_size: 1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "ira_v4_reduction_A/conv1x1_c" 	top: "ira_v4_reduction_A/conv1x1_c" 	name: "bn_ira_v4_reduction_A/conv1x1_c"	type: "BatchNorm"
}

layer {
	bottom: "ira_v4_reduction_A/conv1x1_c" 	top: "ira_v4_reduction_A/conv1x1_c" 	name: "scale_ira_v4_reduction_A/conv1x1_c"	type: "Scale"
	scale_param {	bias_term: true}
}
 layer {
 name: "relu_ira_v4_reduction_A/conv1x1_c" type: "ReLU" bottom: "ira_v4_reduction_A/conv1x1_c" top: "ira_v4_reduction_A/conv1x1_c"
 }

 #-------c__3x3_2----
 layer {
  name: "ira_v4_reduction_A/conv3x3_c"
  type: "Convolution"
  bottom: "ira_v4_reduction_A/conv1x1_c"
  top: "ira_v4_reduction_A/conv3x3_c"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:256
    kernel_size: 3
	pad:1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "ira_v4_reduction_A/conv3x3_c" 	top: "ira_v4_reduction_A/conv3x3_c" 	name: "bn_ira_v4_reduction_A/conv3x3_c"	type: "BatchNorm"
}

layer {
	bottom: "ira_v4_reduction_A/conv3x3_c" 	top: "ira_v4_reduction_A/conv3x3_c" 	name: "scale_ira_v4_reduction_A/conv3x3_c"	type: "Scale"
	scale_param {	bias_term: true}
}
 layer {
 name: "relu_ira_v4_reduction_A/conv3x3_c" type: "ReLU" bottom: "ira_v4_reduction_A/conv3x3_c" top: "ira_v4_reduction_A/conv3x3_c"
 }
 
 #-----3x3_c_reduction
  layer {
  name: "ira_v4_reduction_A/conv3x3_reduction_c"
  type: "Convolution"
  bottom: "ira_v4_reduction_A/conv3x3_c"
  top: "ira_v4_reduction_A/conv3x3_reduction_c"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:384
    kernel_size: 3
	stride: 2
	pad:1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "ira_v4_reduction_A/conv3x3_reduction_c" 	top: "ira_v4_reduction_A/conv3x3_reduction_c" 	name: "bn_ira_v4_reduction_A/conv3x3_reduction_c"	type: "BatchNorm"
}

layer {
	bottom: "ira_v4_reduction_A/conv3x3_reduction_c" 	top: "ira_v4_reduction_A/conv3x3_reduction_c" 	name: "scale_ira_v4_reduction_A/conv3x3_reduction_c"	type: "Scale"
	scale_param {	bias_term: true}
}
 layer {
 name: "relu_ira_v4_reduction_A/conv3x3_reduction_c" type: "ReLU" bottom: "ira_v4_reduction_A/conv3x3_reduction_c" top: "ira_v4_reduction_A/conv3x3_reduction_c"
 }
 ##----reduction A_concat_top ---------------------
  layer{
	 name: "ira_v4_reduction_A/concat"
	 type: "Concat"
	 bottom:"ira_v4_reduction_A/pool"
	 bottom: "ira_v4_reduction_A/conv3x3_reduction_b"
	 bottom: "ira_v4_reduction_A/conv3x3_reduction_c"
	 top: "ira_v4_reduction_A/concat"
 }
#==============================End of REDUCTION_A ========================================



#=========================inception_Res_B==========================================
# the ira_v4_reduction_A/concat of the First module is ira_v4_reduction_A/concat

layer {
  name: "conv4_1b"
  type: "Convolution"
  bottom: "ira_v4_reduction_A/concat"
  top: "conv4_1b"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output: 1154
    kernel_size: 1
    stride: 1 pad:0
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "conv4_1b" 	top: "conv4_1b" 	name: "bn_conv4_1b"	type: "BatchNorm"
	# batch_norm_param {use_global_stats: true}
}

layer {
	bottom: "conv4_1b" 	top: "conv4_1b" 	name: "scale_conv4_1b"	type: "Scale"
	scale_param {	bias_term: true}
}
layer {
   name: "relu4_1b" type: "ReLU" bottom: "conv4_1b" top: "conv4_1b"
  }
  
#-------a_1_1x1----
layer {
                name: "ira_Inception_B_block_1/a_conv1x1_1"
                type: "Convolution"
                bottom: "conv4_1b"
                top: "ira_Inception_B_block_1/a_conv1x1_1"
                param {lr_mult: 1} param {lr_mult: 2}
                convolution_param {
                                num_output:192
                                kernel_size: 1
                                weight_filler { type: "xavier" }
                                bias_filler { type: "constant" value: 0 }
                }
}
layer {
                name: "bn_ira_Inception_B_block_1/a_conv1x1_1" 
                type: "BatchNorm"
                bottom: "ira_Inception_B_block_1/a_conv1x1_1" 
                top: "ira_Inception_B_block_1/a_conv1x1_1"                

}
layer {
                name: "scale_ira_Inception_B_block_1/a_conv1x1_1" 
                type: "Scale"
                bottom: "ira_Inception_B_block_1/a_conv1x1_1"
                top: "ira_Inception_B_block_1/a_conv1x1_1"
                scale_param {    bias_term: true}
}
layer {
                name: "relu_ira_Inception_B_block_1/a_conv1x1_1" 
                type: "ReLU" 
                bottom: "ira_Inception_B_block_1/a_conv1x1_1" 
                top: "ira_Inception_B_block_1/a_conv1x1_1"
}

#-------b1----------
layer {
  name: "ira_Inception_B_block_1/b_conv1x1_1"
  type: "Convolution"
  bottom: "conv4_1b"
  top: "ira_Inception_B_block_1/b_conv1x1_1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:128
    kernel_size: 1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
                name: "bn_ira_Inception_B_block_1/b_conv1x1_1"
                type: "BatchNorm"
                bottom: "ira_Inception_B_block_1/b_conv1x1_1"
                top: "ira_Inception_B_block_1/b_conv1x1_1"
}

layer {
                name: "scale_ira_Inception_B_block_1/b_conv1x1_1" 
                type: "Scale"
                bottom: "ira_Inception_B_block_1/b_conv1x1_1"
                top: "ira_Inception_B_block_1/b_conv1x1_1"
                scale_param {    bias_term: true}
}
layer {
                name: "relu_ira_Inception_B_block_1/b_conv1x1_1"
                type: "ReLU"
               bottom: "ira_Inception_B_block_1/b_conv1x1_1"
                top: "ira_Inception_B_block_1/b_conv1x1_1"
}

#-------b2---------- 
 layer {
  name: "ira_Inception_B_block_1/b_conv1x7_1"
  type: "Convolution"
  bottom: "ira_Inception_B_block_1/b_conv1x1_1"
  top: "ira_Inception_B_block_1/b_conv1x7_1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:160
    kernel_h: 1
    kernel_w: 7
    pad_h: 0
    pad_w: 3
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
                name: "bn_ira_Inception_B_block_1/b_conv1x7_1"
                type: "BatchNorm"
                bottom: "ira_Inception_B_block_1/b_conv1x7_1"
                top: "ira_Inception_B_block_1/b_conv1x7_1"
}

layer {
                name: "scale_ira_Inception_B_block_1/b_conv1x7_1"
                type: "Scale"
                bottom: "ira_Inception_B_block_1/b_conv1x7_1"
                top: "ira_Inception_B_block_1/b_conv1x7_1"
                scale_param {    bias_term: true}
}
layer {
                name: "relu_ira_Inception_B_block_1/b_conv1x7_1"
                type: "ReLU" 
                bottom: "ira_Inception_B_block_1/b_conv1x7_1"
                top: "ira_Inception_B_block_1/b_conv1x7_1"
}


 
#-----------b3-------------------------------
layer {
  name: "ira_Inception_B_block_1/b_conv7x1_1"
  type: "Convolution"
  bottom: "ira_Inception_B_block_1/b_conv1x7_1"
  top: "ira_Inception_B_block_1/b_conv7x1_1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:192
    kernel_h: 7
    kernel_w: 1
    pad_h: 3
    pad_w: 0
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
                name: "bn_ira_Inception_B_block_1/b_conv7x1_1"
                type: "BatchNorm"
                bottom: "ira_Inception_B_block_1/b_conv7x1_1"
                top: "ira_Inception_B_block_1/b_conv7x1_1"
}

layer {
                name: "scale_ira_Inception_B_block_1/b_conv7x1_1"
                type: "Scale"
                bottom: "ira_Inception_B_block_1/b_conv7x1_1"
                top: "ira_Inception_B_block_1/b_conv7x1_1"
                scale_param {    bias_term: true}
}
layer {
                name: "relu_ira_Inception_B_block_1/b_conv7x1_1"
                type: "ReLU" 
                bottom: "ira_Inception_B_block_1/b_conv7x1_1"
                top: "ira_Inception_B_block_1/b_conv7x1_1"
}


#--concatation----------------------- 
 
 layer{
                name: "ira_Inception_B_block_1/concat"
                type: "Concat"
                bottom:"ira_Inception_B_block_1/a_conv1x1_1"
                bottom: "ira_Inception_B_block_1/b_conv7x1_1"
                top: "ira_Inception_B_block_1/concat"
}

#--top conv over paths----------------------- 
  
 layer {
                name: "ira_Inception_B_block_1/top_conv_1x1"
                type: "Convolution"
                bottom: "ira_Inception_B_block_1/concat"
                top: "ira_Inception_B_block_1/top_conv_1x1"
                param {lr_mult: 1} param {lr_mult: 2}
                convolution_param {
                                num_output:1154
                                kernel_size: 1
                                weight_filler { type: "xavier" }
                                bias_filler { type: "constant" value: 0 }
                }
}
layer {
                name: "bn_ira_Inception_B_block_1/top_conv_1x1"
                type: "BatchNorm"
                bottom: "ira_Inception_B_block_1/top_conv_1x1"
                top: "ira_Inception_B_block_1/top_conv_1x1"

}

layer {
                name: "scale_ira_Inception_B_block_1/top_conv_1x1"
                type: "Scale"
                bottom: "ira_Inception_B_block_1/top_conv_1x1"
                top: "ira_Inception_B_block_1/top_conv_1x1"
                scale_param {    bias_term: true}
}


#--Sum  before relu ----------------------- 

layer{
     #name: "ira_Inception_B_block_1/sum"
	  name: "conv4_sum"
     type:  "Eltwise"
     bottom: "conv4_1b"
	 bottom:"ira_Inception_B_block_1/top_conv_1x1"
     #top:"ira_Inception_B_block_1/sum"
	 top: "conv4_sum"
     eltwise_param {operation: SUM coeff: 1 coeff: 0.1}
}
# relu

layer {
                #name: "relu_ira_Inception_B_block_1/sum"
				name: "relu_conv4_sum"
                type: "ReLU" 
				bottom: "conv4_sum"
				top:"conv4_sum"
               # bottom: "ira_Inception_B_block_1/sum"
                #top: "ira_Inception_B_block_1/sum"
}
 #=========================end_inception_Res_B==========================================

 
 ##=================================REDUCTION_B========================



#-----reduction B Pooling a--------------
layer {
  name: "ira_Reduction_B_block_1/a_pool"
  type: "Pooling"
  bottom: "conv4_sum"
  top: "ira_Reduction_B_block_1/a_pool"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}

#-----reduction B b1 conv1x1 --------------
layer {
  name: "ira_Reduction_B_block_1/b_conv1x1_1"
  type: "Convolution"
  bottom: "conv4_sum"
  top: "ira_Reduction_B_block_1/b_conv1x1_1"
 param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output: 256
    kernel_size: 1
                stride:1
                pad:0
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
                name: "bn_ira_Reduction_B_block_1/b_conv1x1_1"
                type: "BatchNorm"
                bottom: "ira_Reduction_B_block_1/b_conv1x1_1"
                top: "ira_Reduction_B_block_1/b_conv1x1_1"
}

layer {
                name: "scale_ira_Reduction_B_block_1/b_conv1x1_1"
                type: "Scale"
                bottom: "ira_Reduction_B_block_1/b_conv1x1_1"
                top: "ira_Reduction_B_block_1/b_conv1x1_1"
                scale_param {    bias_term: true}
}
layer {
                name: "relu_ira_Reduction_B_block_1/b_conv1x1_1"
                type: "ReLU" 
                bottom: "ira_Reduction_B_block_1/b_conv1x1_1"
                top: "ira_Reduction_B_block_1/b_conv1x1_1"
}

#-----reduction B b2 conv3x3--------------
layer {
  name: "ira_Reduction_B_block_1/b_conv3x3_1"
  type: "Convolution"
  bottom: "ira_Reduction_B_block_1/b_conv1x1_1"
  top: "ira_Reduction_B_block_1/b_conv3x3_1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output: 384
    kernel_size: 3
                stride: 2
                pad: 1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
                name: "bn_ira_Reduction_B_block_1/b_conv3x3_1"
                type: "BatchNorm"
                bottom: "ira_Reduction_B_block_1/b_conv3x3_1"
                top: "ira_Reduction_B_block_1/b_conv3x3_1"
}

layer {
                name: "scale_ira_Reduction_B_block_1/b_conv3x3_1"
                type: "Scale"
                bottom: "ira_Reduction_B_block_1/b_conv3x3_1"
                top: "ira_Reduction_B_block_1/b_conv3x3_1"
                scale_param {    bias_term: true}
}
layer {
                name: "relu_ira_Reduction_B_block_1/b_conv3x3_1"
                type: "ReLU" 
                bottom: "ira_Reduction_B_block_1/b_conv3x3_1"
                top: "ira_Reduction_B_block_1/b_conv3x3_1"
}



#-----reduction B c1 conv1x1 --------------
layer {
  name: "ira_Reduction_B_block_1/c_conv1x1_1"
  type: "Convolution"
  bottom: "conv4_sum"
  top: "ira_Reduction_B_block_1/c_conv1x1_1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output: 256
    kernel_size: 1
                stride:1
                pad:0
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
                name: "bn_ira_Reduction_B_block_1/c_conv1x1_1"
                type: "BatchNorm"
                bottom: "ira_Reduction_B_block_1/c_conv1x1_1"
                top: "ira_Reduction_B_block_1/c_conv1x1_1"
}

layer {
                name: "scale_ira_Reduction_B_block_1/c_conv1x1_1"
                type: "Scale"
                bottom: "ira_Reduction_B_block_1/c_conv1x1_1"
                top: "ira_Reduction_B_block_1/c_conv1x1_1"
                scale_param {    bias_term: true}
}
layer {
                name: "relu_ira_Reduction_B_block_1/c_conv1x1_1"
                type: "ReLU" 
                bottom: "ira_Reduction_B_block_1/c_conv1x1_1"
                top: "ira_Reduction_B_block_1/c_conv1x1_1"
}

#-----reduction B c the second conv3x3--------------
layer {
  name: "ira_Reduction_B_block_1/c_conv3x3_1"
  type: "Convolution"
  bottom: "ira_Reduction_B_block_1/c_conv1x1_1"
  top: "ira_Reduction_B_block_1/c_conv3x3_1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output: 288
    kernel_size: 3
                stride: 2
                pad: 1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
                name: "bn_ira_Reduction_B_block_1/c_conv3x3_1"
                type: "BatchNorm"
                bottom: "ira_Reduction_B_block_1/c_conv3x3_1"
                top: "ira_Reduction_B_block_1/c_conv3x3_1"
}

layer {
                name: "scale_ira_Reduction_B_block_1/c_conv3x3_1"
                type: "Scale"
                bottom: "ira_Reduction_B_block_1/c_conv3x3_1"
                top: "ira_Reduction_B_block_1/c_conv3x3_1"
                scale_param {    bias_term: true}
}
layer {
                name: "relu_ira_Reduction_B_block_1/c_conv3x3_1"
                type: "ReLU" 
                bottom: "ira_Reduction_B_block_1/c_conv3x3_1"
                top: "ira_Reduction_B_block_1/c_conv3x3_1"
}


#-----reduction B d1 conv1x1 --------------
layer {
  name: "ira_Reduction_B_block_1/d_conv1x1_1"
  type: "Convolution"
  bottom: "conv4_sum"
  top: "ira_Reduction_B_block_1/d_conv1x1_1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output: 256
    kernel_size: 1
                stride:1
                pad:0
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
                name: "bn_ira_Reduction_B_block_1/d_conv1x1_1"
                type: "BatchNorm"
                bottom: "ira_Reduction_B_block_1/d_conv1x1_1"
                top: "ira_Reduction_B_block_1/d_conv1x1_1"
}

layer {
                name: "scale_ira_Reduction_B_block_1/d_conv1x1_1"
                type: "Scale"
                bottom: "ira_Reduction_B_block_1/d_conv1x1_1"
                top: "ira_Reduction_B_block_1/d_conv1x1_1"
                scale_param {    bias_term: true}
}
layer {
                name: "relu_ira_Reduction_B_block_1/d_conv1x1_1"
                type: "ReLU" 
                bottom: "ira_Reduction_B_block_1/d_conv1x1_1"
                top: "ira_Reduction_B_block_1/d_conv1x1_1"
}

#-----reduction B d the second  conv3x3--------------
layer {
  name: "ira_Reduction_B_block_1/d_conv3x3_1"
  type: "Convolution"
  bottom: "ira_Reduction_B_block_1/d_conv1x1_1"
  top: "ira_Reduction_B_block_1/d_conv3x3_1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output: 288
    kernel_size: 3
                stride: 1
                pad: 1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
                name: "bn_ira_Reduction_B_block_1/d_conv3x3_1"
                type: "BatchNorm"
                bottom: "ira_Reduction_B_block_1/d_conv3x3_1"
                top: "ira_Reduction_B_block_1/d_conv3x3_1"
}

layer {
                name: "scale_ira_Reduction_B_block_1/d_conv3x3_1"
                type: "Scale"
                bottom: "ira_Reduction_B_block_1/d_conv3x3_1"
                top: "ira_Reduction_B_block_1/d_conv3x3_1"
                scale_param {    bias_term: true}
}
layer {
                name: "relu_ira_Reduction_B_block_1/d_conv3x3_1"
                type: "ReLU" 
                bottom: "ira_Reduction_B_block_1/d_conv3x3_1"
                top: "ira_Reduction_B_block_1/d_conv3x3_1"
}

#-----reduction B d the second conv3x3--------------
layer {
  name: "ira_Reduction_B_block_1/d_conv3x3_2"
  type: "Convolution"
  bottom: "ira_Reduction_B_block_1/d_conv3x3_1"
  top: "ira_Reduction_B_block_1/d_conv3x3_2"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output: 320
    kernel_size: 3
                stride: 2
                pad: 1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
                name: "bn_ira_Reduction_B_block_1/d_conv3x3_2"
                type: "BatchNorm"
                bottom: "ira_Reduction_B_block_1/d_conv3x3_2"
                top: "ira_Reduction_B_block_1/d_conv3x3_2"
}

layer {
                name: "scale_ira_Reduction_B_block_1/d_conv3x3_2"
                type: "Scale"
                bottom: "ira_Reduction_B_block_1/d_conv3x3_2"
                top: "ira_Reduction_B_block_1/d_conv3x3_2"
                scale_param {    bias_term: true}
}
layer {
                name: "relu_ira_Reduction_B_block_1/d_conv3x3_2"
                type: "ReLU" 
                bottom: "ira_Reduction_B_block_1/d_conv3x3_2"
                top: "ira_Reduction_B_block_1/d_conv3x3_2"
}




##----reduction B_concat_top ---------------------
layer{
                name: "ira_Reduction_B_block_1/concat"
                type: "Concat"
                bottom:"ira_Reduction_B_block_1/a_pool"
                bottom: "ira_Reduction_B_block_1/b_conv3x3_1"
                bottom: "ira_Reduction_B_block_1/c_conv3x3_1"
                bottom: "ira_Reduction_B_block_1/d_conv3x3_2"
                top: "ira_Reduction_B_block_1/concat"
}
#==============================End of REDUCTION_B ========================================

 
 
 
#=========================inception_Res_C==========================================
# the input of the First module is input

layer {
  name: "conv5_1b"
  type: "Convolution"
  bottom: "ira_Reduction_B_block_1/concat"
  top: "conv5_1b"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output: 2048
    kernel_size: 1
    stride: 1 pad:0
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
	bottom: "conv5_1b" 	top: "conv5_1b" 	name: "bn_conv5_1b"	type: "BatchNorm"
	# batch_norm_param {use_global_stats: true}
}

layer {
	bottom: "conv5_1b" 	top: "conv5_1b" 	name: "scale_conv5_1b"	type: "Scale"
	scale_param {	bias_term: true}
}
layer {
   name: "relu5_1b" type: "ReLU" bottom: "conv5_1b" top: "conv5_1b"
  }
  
#-------a_1_1x1----
layer {
                name: "ira_Inception_C_block_1/a_conv1x1_1"
                type: "Convolution"
                bottom: "conv5_1b"
                top: "ira_Inception_C_block_1/a_conv1x1_1"
                param {lr_mult: 1} param {lr_mult: 2}
                convolution_param {
                                num_output:192
                                kernel_size: 1
                                weight_filler { type: "xavier" }
                                bias_filler { type: "constant" value: 0 }
                }
}
layer {
                name: "bn_ira_Inception_C_block_1/a_conv1x1_1" 
                type: "BatchNorm"
                bottom: "ira_Inception_C_block_1/a_conv1x1_1" 
                top: "ira_Inception_C_block_1/a_conv1x1_1"                

}
layer {
                name: "scale_ira_Inception_C_block_1/a_conv1x1_1" 
                type: "Scale"
                bottom: "ira_Inception_C_block_1/a_conv1x1_1"
                top: "ira_Inception_C_block_1/a_conv1x1_1"
                scale_param {    bias_term: true}
}
layer {
                name: "relu_ira_Inception_C_block_1/a_conv1x1_1" 
                type: "ReLU" 
                bottom: "ira_Inception_C_block_1/a_conv1x1_1" 
                top: "ira_Inception_C_block_1/a_conv1x1_1"
}

#-------b1----------
layer {
  name: "ira_Inception_C_block_1/b_conv1x1_1"
  type: "Convolution"
  bottom: "conv5_1b"
  top: "ira_Inception_C_block_1/b_conv1x1_1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:192
    kernel_size: 1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
                name: "bn_ira_Inception_C_block_1/b_conv1x1_1"
                type: "BatchNorm"
                bottom: "ira_Inception_C_block_1/b_conv1x1_1"
                top: "ira_Inception_C_block_1/b_conv1x1_1"
}

layer {
                name: "scale_ira_Inception_C_block_1/b_conv1x1_1" 
                type: "Scale"
                bottom: "ira_Inception_C_block_1/b_conv1x1_1"
                top: "ira_Inception_C_block_1/b_conv1x1_1"
                scale_param {    bias_term: true}
}
layer {
                name: "relu_ira_Inception_C_block_1/b_conv1x1_1"
                type: "ReLU"
               bottom: "ira_Inception_C_block_1/b_conv1x1_1"
                top: "ira_Inception_C_block_1/b_conv1x1_1"
}

#-------b2---------- 
 layer {
  name: "ira_Inception_C_block_1/b_conv1x7_1"
  type: "Convolution"
  bottom: "ira_Inception_C_block_1/b_conv1x1_1"
  top: "ira_Inception_C_block_1/b_conv1x7_1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:224
    kernel_h: 1
    kernel_w: 3
    pad_h: 0
    pad_w: 1
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
                name: "bn_ira_Inception_C_block_1/b_conv1x7_1"
                type: "BatchNorm"
                bottom: "ira_Inception_C_block_1/b_conv1x7_1"
                top: "ira_Inception_C_block_1/b_conv1x7_1"
}

layer {
                name: "scale_ira_Inception_C_block_1/b_conv1x7_1"
                type: "Scale"
                bottom: "ira_Inception_C_block_1/b_conv1x7_1"
                top: "ira_Inception_C_block_1/b_conv1x7_1"
                scale_param {    bias_term: true}
}
layer {
                name: "relu_ira_Inception_C_block_1/b_conv1x7_1"
                type: "ReLU" 
                bottom: "ira_Inception_C_block_1/b_conv1x7_1"
                top: "ira_Inception_C_block_1/b_conv1x7_1"
}


 
#-----------b3-------------------------------
layer {
  name: "ira_Inception_C_block_1/b_conv7x1_1"
  type: "Convolution"
  bottom: "ira_Inception_C_block_1/b_conv1x7_1"
  top: "ira_Inception_C_block_1/b_conv7x1_1"
  param {lr_mult: 1} param {lr_mult: 2}
  convolution_param {
    num_output:256
    kernel_h: 3
    kernel_w: 1
    pad_h: 1
    pad_w: 0
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
                name: "bn_ira_Inception_C_block_1/b_conv7x1_1"
                type: "BatchNorm"
                bottom: "ira_Inception_C_block_1/b_conv7x1_1"
                top: "ira_Inception_C_block_1/b_conv7x1_1"
}

layer {
                name: "scale_ira_Inception_C_block_1/b_conv7x1_1"
                type: "Scale"
                bottom: "ira_Inception_C_block_1/b_conv7x1_1"
                top: "ira_Inception_C_block_1/b_conv7x1_1"
                scale_param {    bias_term: true}
}
layer {
                name: "relu_ira_Inception_C_block_1/b_conv7x1_1"
                type: "ReLU" 
                bottom: "ira_Inception_C_block_1/b_conv7x1_1"
                top: "ira_Inception_C_block_1/b_conv7x1_1"
}


#--concatation----------------------- 
 
 layer{
                name: "ira_Inception_C_block_1/concat"
                type: "Concat"
                bottom:"ira_Inception_C_block_1/a_conv1x1_1"
                bottom: "ira_Inception_C_block_1/b_conv7x1_1"
                top: "ira_Inception_C_block_1/concat"
}

#--top conv over paths----------------------- 
  
 layer {
                name: "ira_Inception_C_block_1/top_conv_1x1"
                type: "Convolution"
                bottom: "ira_Inception_C_block_1/concat"
                top: "ira_Inception_C_block_1/top_conv_1x1"
                param {lr_mult: 1} param {lr_mult: 2}
                convolution_param {
                                num_output:2048
                                kernel_size: 1
                                weight_filler { type: "xavier" }
                                bias_filler { type: "constant" value: 0 }
                }
}
layer {
                name: "bn_ira_Inception_C_block_1/top_conv_1x1"
                type: "BatchNorm"
                bottom: "ira_Inception_C_block_1/top_conv_1x1"
                top: "ira_Inception_C_block_1/top_conv_1x1"

}

layer {
                name: "scale_ira_Inception_C_block_1/top_conv_1x1"
                type: "Scale"
                bottom: "ira_Inception_C_block_1/top_conv_1x1"
                top: "ira_Inception_C_block_1/top_conv_1x1"
                scale_param {    bias_term: true}
}


#--Sum  before relu ----------------------- 

layer{
     #name: "ira_Inception_C_block_1/sum"
	  name: "conv5_sum"
     type:  "Eltwise"
     bottom: "conv5_1b"
	 bottom:"ira_Inception_C_block_1/top_conv_1x1"
     #top:"ira_Inception_C_block_1/sum"
	 top: "conv5_sum"
     eltwise_param {operation: SUM coeff: 1 coeff: 0.1}
}
# relu

layer {
                #name: "relu_ira_Inception_C_block_1/sum"
				name: "relu_conv5_sum"
                type: "ReLU" 
				bottom: "conv5_sum"
				top:"conv5_sum"
               # bottom: "ira_Inception_C_block_1/sum"
                #top: "ira_Inception_C_block_1/sum"
}
 #=========================end_inception_Res_B==========================================







# --------------------conv 4_X----------------------------





#########-----------------------------conv5x_____----------------------------------------------

#---conv 4_X----------------------------


# layer {
  # name: "pool5"
  # type: "Pooling"
  # bottom: "conv4_sum"
  # top: "pool5"
  # pooling_param {
    # pool: MAX
  # kernel_size: 2
  # stride: 2
  # }
# } 

# layer {
  # name: "conv5_1b"
  # type: "Convolution"
  # bottom: "ira_Reduction_B_block_1/concat"
  # top: "conv5_1b"
  # param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param {
    # num_output: 1024
    # kernel_size: 1
    # stride: 1 pad:0
    # weight_filler { type: "xavier" }
    # bias_filler { type: "constant" value: 0 }
  # }
# }
# layer {
	# bottom: "conv5_1b" 	top: "conv5_1b" 	name: "bn_conv5_1b"	type: "BatchNorm"
	# # batch_norm_param {use_global_stats: true}
# }

# layer {
	# bottom: "conv5_1b" 	top: "conv5_1b" 	name: "scale_conv5_1b"	type: "Scale"
	# scale_param {	bias_term: true}
# }
# layer {
   # name: "relu5_1b" type: "ReLU" bottom: "conv5_1b" top: "conv5_1b"

 # }

# layer {
  # name: "conv5_1"
  # type: "Convolution"
  # bottom: "ira_Reduction_B_block_1/concat"
  # top: "conv5_1"
  # param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param {
    # num_output: 1024
    # kernel_size: 3
    # stride: 1 pad:1
    # weight_filler { type: "xavier" }
    # bias_filler { type: "constant" value: 0 }
  # }
# }

# layer {
	# bottom: "conv5_1" 	top: "conv5_1" 	name: "bn_conv5_1"	type: "BatchNorm"
	# # batch_norm_param {use_global_stats: true}
# }

# layer {
	# bottom: "conv5_1" 	top: "conv5_1" 	name: "scale_conv5_1"	type: "Scale"
	# scale_param {	bias_term: true}
# }
# layer {
  # name: "relu4_1" type: "ReLU" bottom: "conv5_1" top: "conv5_1"
# }





# layer {
  # name: "conv5_2"
  # type: "Convolution"
  # bottom: "conv5_1"
  # top: "conv5_2"
  # param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param {
    # num_output: 1024
    # kernel_size: 3
    # stride: 1 pad:1
    # weight_filler { type: "xavier" }
    # bias_filler { type: "constant" value: 0 }
  # }
# }

# layer {
	# bottom: "conv5_2" 	top: "conv5_2" 	name: "bn_conv5_2"	type: "BatchNorm"
	# # batch_norm_param {use_global_stats: true}
# }

# layer {
	# bottom: "conv5_2" 	top: "conv5_2" 	name: "scale_conv5_2"	type: "Scale"
	# scale_param {	bias_term: true}
# }
# layer {
  # name: "relu4_2" type: "ReLU" bottom: "conv5_2" top: "conv5_2"
# }

# layer {
  # name: "conv5_3"
  # type: "Convolution"
  # bottom: "conv5_2"
  # top: "conv5_3"
  # param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param {
    # num_output: 1024
    # kernel_size: 3
    # stride: 1 pad:1
    # weight_filler { type: "xavier" }
    # bias_filler { type: "constant" value: 0 }
  # }
# }
# layer {
	# bottom: "conv5_3" 	top: "conv5_3" 	name: "bn_conv5_3"	type: "BatchNorm"
	# # batch_norm_param {use_global_stats: true}
# }

# layer {
	# bottom: "conv5_3" 	top: "conv5_3" 	name: "scale_conv5_3"	type: "Scale"
	# scale_param {	bias_term: true}
# }



# layer{
     # name: "conv5_sum"
     # type:  "Eltwise"
     # bottom:"conv5_3"
     # bottom: "conv5_1b"
     # top:"conv5_sum"
     # eltwise_param {operation: SUM   }
# }


# layer {name: "relu5_sum" type: "ReLU" bottom: "conv5_sum" top: "conv5_sum"}

# layer {
  # name: "pool5"
  # type: "Pooling"
  # bottom: "conv5_sum"
  # top: "pool5"
  # top: "pool5_mask"
  # pooling_param {
    # pool: MAX
    # kernel_size: 2
    # stride: 2
  # }
# }






#----------------- devconv to xx times -------------------------------------------------------



# layer { type: "Unpooling"  bottom: "pool5"  bottom: "pool5_mask"  top: "unpool5"  name: "unpool5"
  # unpooling_param {   unpool: MAX   kernel_size: 2    stride: 2 pad:0}
# }


# layer { bottom: 'unpool5' top: 'deconv5_2b' name: 'deconv5_2b'  type: "Deconvolution"
  # param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param { engine:CUDNN num_output: 128	pad: 0	kernel_size: 1
    # weight_filler {      type: "gaussian"      std: 0.01    }
# bias_filler {      type: "constant"      value: 0    }} }
# layer {
	# bottom: "deconv5_2b" 	top: "deconv5_2b" 	name: "bn_deconv5_2b"	type: "BatchNorm"
# #	# batch_norm_param {use_global_stats: true}
# }

# layer {
	# bottom: "deconv5_2b" 	top: "deconv5_2b" 	name: "scale_deconv5_2b"	type: "Scale"
	# scale_param {	bias_term: true}
# }
# layer {
   # name: "relu4_2b" type: "ReLU" bottom: "deconv5_2b" top: "deconv5_2b"
 # }




# layer { bottom: 'unpool5' top: 'deconv5_2' name: 'deconv5_2'  type: "Deconvolution"
  # param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param { engine:CUDNN num_output: 128	pad: 1	kernel_size: 3
    # weight_filler {      type: "gaussian"      std: 0.01    }
# bias_filler {      type: "constant"      value: 0    }} }

# layer {
	# bottom: "deconv5_2" 	top: "deconv5_2" 	name: "bn_deconv5_2"	type: "BatchNorm"
# #	# batch_norm_param {use_global_stats: true}
# }

# layer {
	# bottom: "deconv5_2" 	top: "deconv5_2" 	name: "scale_deconv5_2"	type: "Scale"
	# scale_param {	bias_term: true}
# }
# layer {
  # name: "relu4_2" type: "ReLU" bottom: "deconv5_2" top: "deconv5_2"
# }


# layer { bottom: 'deconv5_2' top: 'deconv5_1' name: 'deconv5_1'  type: "Deconvolution"
# param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param { engine:CUDNN num_output: 128	pad: 1	kernel_size: 3
    # weight_filler {      type: "gaussian"      std: 0.01    }
# bias_filler {      type: "constant"      value: 0    }} }
# layer {
	# bottom: "deconv5_1" 	top: "deconv5_1" 	name: "bn_deconv5_1"	type: "BatchNorm"
# #	# batch_norm_param {use_global_stats: true}
# }

# layer {
	# bottom: "deconv5_1" 	top: "deconv5_1" 	name: "scale_deconv5_1"	type: "Scale"
	# scale_param {	bias_term: true}
# }



# layer{
     # name: "deconv5_sum" type:  "Eltwise" bottom:"deconv5_2b" bottom: "deconv5_1" top:"deconv5_sum"
     # eltwise_param {operation: SUM   }
     # }
     # layer {
       # name: "relu4_deconv5_sum" type: "ReLU" bottom: "deconv5_sum" top: "deconv5_sum"
     # }





# #------------------deconv 4-----------------------------------------------------
# # layer { bottom: 'unpool4' top: 'deconv4_3' name: 'deconv4_3'  type: "Deconvolution"
# #   param {lr_mult: 1} param {lr_mult: 2}
# #   convolution_param { num_output: 512	pad: 1	kernel_size: 3
# #     weight_filler {      type: "gaussian"      std: 0.01    }
# # bias_filler {      type: "constant"      value: 0    }} }
# #
# # layer { bottom: 'deconv4_3' top: 'deconv4_3' name: 'derelu4_3' type: "ReLU" }



# layer { type: "Unpooling"  bottom: "deconv5_sum"  bottom: "pool4_mask"  top: "unpool4"  name: "unpool4"
  # unpooling_param {   unpool: MAX   kernel_size: 2    stride: 2 pad:0}
# }

# layer { bottom: 'unpool4' top: 'deconv4_2b' name: 'deconv4_2b'  type: "Deconvolution"
  # param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param { engine:CUDNN num_output: 128	pad: 0	kernel_size: 1
    # weight_filler {      type: "gaussian"      std: 0.01    }
# bias_filler {      type: "constant"      value: 0    }} }
# layer {
	# bottom: "deconv4_2b" 	top: "deconv4_2b" 	name: "bn_deconv4_2b"	type: "BatchNorm"
# #	# batch_norm_param {use_global_stats: true}
# }

# layer {
	# bottom: "deconv4_2b" 	top: "deconv4_2b" 	name: "scale_deconv4_2b"	type: "Scale"
	# scale_param {	bias_term: true}
# }
# layer {
    # name: "relu4_2b" type: "ReLU" bottom: "deconv4_2b" top: "deconv4_2b"
# }




# layer { bottom: 'unpool4' top: 'deconv4_2' name: 'deconv4_2'  type: "Deconvolution"
  # param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param { engine:CUDNN num_output: 128	pad: 1	kernel_size: 3
    # weight_filler {      type: "gaussian"      std: 0.01    }
# bias_filler {      type: "constant"      value: 0    }} }

# layer {
	# bottom: "deconv4_2" 	top: "deconv4_2" 	name: "bn_deconv4_2"	type: "BatchNorm"
# #	# batch_norm_param {use_global_stats: true}
# }

# layer {
	# bottom: "deconv4_2" 	top: "deconv4_2" 	name: "scale_deconv4_2"	type: "Scale"
	# scale_param {	bias_term: true}
# }
# layer {
  # name: "relu4_2" type: "ReLU" bottom: "deconv4_2" top: "deconv4_2"
# }


# layer { bottom: 'deconv4_2' top: 'deconv4_1' name: 'deconv4_1'  type: "Deconvolution"
# param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param { engine:CUDNN num_output: 128	pad: 1	kernel_size: 3
    # weight_filler {      type: "gaussian"      std: 0.01    }
# bias_filler {      type: "constant"      value: 0    }} }
# layer {
	# bottom: "deconv4_1" 	top: "deconv4_1" 	name: "bn_deconv4_1"	type: "BatchNorm"
# #	# batch_norm_param {use_global_stats: true}
# }

# layer {
	# bottom: "deconv4_1" 	top: "deconv4_1" 	name: "scale_deconv4_1"	type: "Scale"
	# scale_param {	bias_term: true}
# }



# layer{
     # name: "deconv4_sum" type:  "Eltwise" bottom:"deconv4_2b" bottom: "deconv4_1" top:"deconv4_sum"
     # eltwise_param {operation:SUM   }
     # }
     # # layer {
     	# # bottom: "deconv4_sum" 	top: "deconv4_sum" 	name: "bn_deconv4_sum"	type: "BatchNorm"
     # # #	# batch_norm_param {use_global_stats: true}
     # # }

     # # layer {
     	# # bottom: "deconv4_sum" 	top: "deconv4_sum" 	name: "scale_deconv4_sum"	type: "Scale"
     	# # scale_param {	bias_term: true}
     # # }
     # layer {
       # name: "relu4_deconv4_sum" type: "ReLU" bottom: "deconv4_sum" top: "deconv4_sum"
     # }


# #-------- deconv_3-------------------------------------

# layer { type: "Unpooling"  bottom: "deconv4_sum"  bottom: "pool3_mask"  top: "unpool3"  name: "unpool3"
  # unpooling_param {   unpool: MAX   kernel_size: 2    stride: 2 pad:0}
# }

# layer { bottom: 'unpool3' top: 'deconv3_3b' name: 'deconv3_3b'  type: "Deconvolution"
  # param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param { engine:CUDNN num_output: 128	pad: 0	kernel_size: 1
    # weight_filler {      type: "gaussian"      std: 0.01    }
# bias_filler {      type: "constant"      value: 0    }} }
# layer {
	# bottom: "deconv3_3b" 	top: "deconv3_3b" 	name: "bn_deconv3_3b"	type: "BatchNorm"
# #	# batch_norm_param {use_global_stats: true}
# }

# layer {
	# bottom: "deconv3_3b" 	top: "deconv3_3b" 	name: "scale_deconv3_3b"	type: "Scale"
	# scale_param {	bias_term: true}
# }
# layer {
   # name: "relu_deconv3_3b" type: "ReLU" bottom: "deconv3_3b" top: "deconv3_3b"
# }


# layer { bottom: 'unpool3' top: 'deconv3_3' name: 'deconv3_3'  type: "Deconvolution"
  # param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param { engine:CUDNN num_output: 128	pad: 1	kernel_size: 3
    # weight_filler {      type: "gaussian"      std: 0.01    }
# bias_filler {      type: "constant"      value: 0    }} }
# layer {
	# bottom: "deconv3_3" 	top: "deconv3_3" 	name: "bn_deconv3_3"	type: "BatchNorm"
# #	# batch_norm_param {use_global_stats: true}
# }
# layer {
	# bottom: "deconv3_3" 	top: "deconv3_3" 	name: "scale_deconv3_3"	type: "Scale"
	# scale_param {	bias_term: true}
# }
# layer {
  # name: "relu_deconv3_3" type: "ReLU" bottom: "deconv3_3" top: "deconv3_3"
# }


# layer { bottom: 'deconv3_3' top: 'deconv3_2' name: 'deconv3_2'  type: "Deconvolution"
  # param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param { engine:CUDNN num_output: 128	pad: 1	kernel_size: 3
    # weight_filler {      type: "gaussian"      std: 0.01    }
# bias_filler {      type: "constant"      value: 0    }} }
# layer {
	# bottom: "deconv3_2" 	top: "deconv3_2" 	name: "bn_deconv3_2"	type: "BatchNorm"
# #	# batch_norm_param {use_global_stats: true}
# }

# layer {
	# bottom: "deconv3_2" 	top: "deconv3_2" 	name: "scale_deconv3_2"	type: "Scale"
	# scale_param {	bias_term: true}
# }

# layer{
     # name: "deconv3_sum" type:  "Eltwise" bottom:"deconv3_3b" bottom: "deconv3_2" top:"deconv3_sum"
     # eltwise_param {operation: SUM   }
     # }

     # # layer {
     	# # bottom: "deconv3_sum" 	top: "deconv3_sum" 	name: "bn_deconv3_sum"	type: "BatchNorm"
     # # #	# batch_norm_param {use_global_stats: true}
     # # }

     # # layer {
     	# # bottom: "deconv3_sum" 	top: "deconv3_sum" 	name: "scale_deconv3_sum"	type: "Scale"
     	# # scale_param {	bias_term: true}
     # # }
     # layer {
       # name: "relu3_sum" type: "ReLU" bottom: "deconv3_sum" top: "deconv3_sum"
     # }
# #-------- deconv_2-------------------------------------

# layer { type: "Unpooling"  bottom: "deconv3_sum"  bottom: "pool2_mask"  top: "unpool2"  name: "unpool2"
  # unpooling_param {   unpool: MAX   kernel_size: 2    stride: 2 }
# }

# layer { bottom: 'unpool2' top: 'deconv2_3b' name: 'deconv2_3b'  type: "Deconvolution"
  # param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param { engine:CUDNN num_output: 64	pad: 0	kernel_size: 1
    # weight_filler {      type: "gaussian"      std: 0.01    }
# bias_filler {      type: "constant"      value: 0    }} }
# layer {
	# bottom: "deconv2_3b" 	top: "deconv2_3b" 	name: "bn_deconv2_3b"	type: "BatchNorm"
# #	# batch_norm_param {use_global_stats: true}
# }

# layer {
	# bottom: "deconv2_3b" 	top: "deconv2_3b" 	name: "scale_deconv2_3b"	type: "Scale"
	# scale_param {	bias_term: true}
# }
# layer {
   # name: "relu_deconv2_3b" type: "ReLU" bottom: "deconv2_3b" top: "deconv2_3b"
 # }

# layer { bottom: 'unpool2' top: 'deconv2_3' name: 'deconv2_3'  type: "Deconvolution"
  # param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param {engine:CUDNN num_output: 64	pad: 1	kernel_size: 3
    # weight_filler {      type: "gaussian"      std: 0.01    }
# bias_filler {      type: "constant"      value: 0    }} }
# layer {
	# bottom: "deconv2_3" 	top: "deconv2_3" 	name: "bn_deconv2_3"	type: "BatchNorm"
# #	# batch_norm_param {use_global_stats: true}
# }

# layer {
	# bottom: "deconv2_3" 	top: "deconv2_3" 	name: "scale_deconv2_3"	type: "Scale"
	# scale_param {	bias_term: true}
# }
# layer {
  # name: "relu_deconv2_3" type: "ReLU" bottom: "deconv2_3" top: "deconv2_3"
# }

# layer { bottom: 'deconv2_3' top: 'deconv2_2' name: 'deconv2_2'  type: "Deconvolution"
  # param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param { engine:CUDNN num_output: 64	pad: 1	kernel_size: 3
    # weight_filler {      type: "gaussian"      std: 0.01    }
# bias_filler {      type: "constant"      value: 0    }} }
# layer {
	# bottom: "deconv2_2" 	top: "deconv2_2" 	name: "bn_deconv2_2"	type: "BatchNorm"
# #	# batch_norm_param {use_global_stats: true}
# }

# layer {
	# bottom: "deconv2_2" 	top: "deconv2_2" 	name: "scale_deconv2_2"	type: "Scale"
	# scale_param {	bias_term: true}
# }

# layer{
     # name: "deconv2_sum" type:  "Eltwise" bottom:"deconv2_3b" bottom: "deconv2_2" top:"deconv2_sum"
     # eltwise_param {operation: SUM   }
     # }

     # # layer {
     	# # bottom: "deconv2_sum" 	top: "deconv2_sum" 	name: "bn_deconv2_sum"	type: "BatchNorm"
     # # #	# batch_norm_param {use_global_stats: true}
     # # }

     # # layer {
     	# # bottom: "deconv2_sum" 	top: "deconv2_sum" 	name: "scale_deconv2_sum"	type: "Scale"
     	# # scale_param {	bias_term: true}
     # # }
     # layer {
       # name: "relu2_sum" type: "ReLU" bottom: "deconv2_sum" top: "deconv2_sum"
     # }


# #-------- deconv_1-------------------------------------

# #layer { type: "Unpooling"  bottom: "deconv2_sum"  bottom: "pool1_mask"  top: "unpool1"  name: "unpool1"
# #  unpooling_param {   unpool: MAX   kernel_size: 2    stride: 2 }
# #}
 
 # # layer { bottom: 'unpool1' top: 'deconv1_3b' name: 'deconv1_3b'  type: "Deconvolution"
  # # param {lr_mult: 1} param {lr_mult: 2}
  # # convolution_param { num_output: 64	pad: 0	kernel_size: 1
    # # weight_filler {      type: "gaussian"      std: 0.01    }
# # bias_filler {      type: "constant"      value: 0    }} } 



# layer { bottom: 'deconv2_sum' top: 'deconv1_3b' name: 'deconv1_3b'  type: "Deconvolution"
  # param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param { engine:CUDNN num_output: 64	pad: 1	kernel_size: 4 stride:2
    # weight_filler {      type: "gaussian"      std: 0.01    }
# bias_filler {      type: "constant"      value: 0    }} }
# layer {
	# bottom: "deconv1_3b" 	top: "deconv1_3b" 	name: "bn_deconv1_3b"	type: "BatchNorm"
# #	# batch_norm_param {use_global_stats: true}
# }

# layer {
	# bottom: "deconv1_3b" 	top: "deconv1_3b" 	name: "scale_deconv1_3b"	type: "Scale"
	# scale_param {	bias_term: true}
# }
# # layer {
  # # name: "relu1_3b" type: "ReLU" bottom: "deconv1_3b" top: "deconv1_3b" }


# layer { bottom: 'deconv2_sum' top: 'deconv1_3' name: 'deconv1_3'  type: "Deconvolution"
  # param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param {engine:CUDNN num_output: 64	pad: 3	kernel_size: 8 stride:2
    # weight_filler {      type: "gaussian"      std: 0.01    }
# bias_filler {      type: "constant"      value: 0    }} }
# layer {
	# bottom: "deconv1_3" 	top: "deconv1_3" 	name: "bn_deconv1_3"	type: "BatchNorm"
# #	# batch_norm_param {use_global_stats: true}
# }

# layer {
	# bottom: "deconv1_3" 	top: "deconv1_3" 	name: "scale_deconv1_3"	type: "Scale"
	# scale_param {	bias_term: true}
# }
# layer {
  # name: "relu1_3" type: "ReLU" bottom: "deconv1_3" top: "deconv1_3"
# }


# layer { bottom: 'deconv1_3' top: 'deconv1_2' name: 'deconv1_2'  type: "Deconvolution"
  # param {lr_mult: 1} param {lr_mult: 2}
  # convolution_param { engine:CUDNN num_output: 64	pad: 1	kernel_size: 3
    # weight_filler {      type: "gaussian"      std: 0.01    }
# bias_filler {      type: "constant"      value: 0    }} }

# layer {
	# bottom: "deconv1_2" 	top: "deconv1_2" 	name: "bn_deconv1_2"	type: "BatchNorm"
# #	# batch_norm_param {use_global_stats: true}
# }

# layer {
	# bottom: "deconv1_2" 	top: "deconv1_2" 	name: "scale_deconv1_2"	type: "Scale"
	# scale_param {	bias_term: true}
# }

# layer{
     # name: "deconv1_sum" type:  "Eltwise" bottom:"deconv1_2" bottom: "deconv1_3b" top:"deconv1_sum"
     # eltwise_param {operation: SUM   }
# }

# # layer {
	# # bottom: "deconv1_sum" 	top: "deconv1_sum" 	name: "bn_deconv1_sum"	type: "BatchNorm"
# # #	# batch_norm_param {use_global_stats: true}
# # }

# # layer {
	# # bottom: "deconv1_sum" 	top: "deconv1_sum" 	name: "scale_deconv1_sum"	type: "Scale"
	# # scale_param {	bias_term: true}
# # }
# layer {
  # name: "relu1_sum" type: "ReLU" bottom: "deconv1_sum" top: "deconv1_sum"
# }


# # layer { name: 'seg_score' type: "Deconvolution" bottom: 'deconv1_sum' top: 'seg_score'
  # # param {lr_mult: 1} param {lr_mult: 2}
  # # convolution_param { num_output: 2 kernel_size: 4 stride:2
    # # weight_filler {      type: "gaussian"      std: 0.01    }
    # # bias_filler {      type: "constant"      value: 0    }} }



# layer { name: 'seg_score' type: "Convolution" bottom: 'deconv1_sum' top: 'seg_score'
  # param {lr_mult: 1} param {lr_mult: 2}
   # convolution_param { num_output: 2 kernel_size: 1
     # weight_filler {      type: "gaussian"      std: 0.01    }
     # bias_filler {      type: "constant"      value: 0    }} }




    # ----------------------------------deconv once---------------------------
          layer{
         name: "deconv5_16x"
           type: "Deconvolution"
           bottom: "conv5_sum"
           top: "deconv5_16x"
          param {lr_mult: 1} param {lr_mult: 2}
           convolution_param {
             num_output: 2 kernel_size: 32 stride: 16 pad:8
             weight_filler {type: "bilinear"}
           }
         }
		 
         # layer{
         # name: "deconv4_8x"
           # type: "Deconvolution"
           # bottom: "conv4_sum"
           # top: "deconv4_8x"
          # param {lr_mult: 1} param {lr_mult: 2}
           # convolution_param {
             # num_output: 2 kernel_size: 16 stride: 8 pad:4
             # weight_filler {type: "bilinear"}
           # }
         # }

         # layer{
         # name: "deconv3_4x"
           # type: "Deconvolution"
           # bottom: "conv3_sum"
           # top: "deconv3_4x"
           # param {lr_mult: 1} param {lr_mult: 2}
            # convolution_param {
              # num_output: 2 kernel_size: 8 stride: 4 pad:2
              # weight_filler {type: "bilinear"}
            # }
         # }

         # layer{
         # name: "deconv2_2x"
           # type: "Deconvolution"
           # bottom: "concat_stem_1"
           # top: "deconv2_2x"
           # param {lr_mult: 1} param {lr_mult: 2}
            # convolution_param {
              # num_output: 2 kernel_size: 4 stride: 2 pad:1
              # weight_filler {type: "bilinear"}
            # }
         # }
		 
		 
# ============================ Dialated multiple upsampling as classifier for 2x 4x and 8x , leave 16x for single deconvolution ==============================		 
  layer{
         name: "deconv2_2x_d1"
           type: "Deconvolution"
           bottom: "concat_stem_1"
           top: "deconv2_2x_d1"
           param {lr_mult: 1} param {lr_mult: 2}
            convolution_param {
              num_output: 2 kernel_size: 4 stride: 2 pad:1
              weight_filler {type: "bilinear"}
            }
         }

layer {
  name: "fc1_2x_c0"
  type: "Deconvolution"
  bottom: "concat_stem_1"
  top: "deconv2_2x_c0"
  param {
    name: "fc1_2x_c0_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc1_2x_c0_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 2
    kernel_size: 4
	stride: 2	
    pad: 7
    dilation:5
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

layer {
  name: "fc1_2x_c1"
  type: "Deconvolution"
  bottom: "concat_stem_1"
  top: "deconv2_2x_c1"
  param {
    name: "fc1_2x_c1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc1_2x_c1_b"
    lr_mult: 2
    decay_mult: 0
  }
convolution_param {
    num_output: 2
    kernel_size: 4
	stride: 2	
    pad: 16
    dilation: 11
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

layer {
  name: "fc1_2x_c2"
   type: "Deconvolution"
  bottom: "concat_stem_1"
  top: "deconv2_2x_c2"
  param {
    name: "fc1_2x_c2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc1_2x_c2_b"
    lr_mult: 2
    decay_mult: 0
  }
convolution_param {
    num_output: 2
    kernel_size: 4
	stride: 2	
    pad: 25
    dilation: 17
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

layer {
  name: "fc1_2x_c3"
   type: "Deconvolution"
  bottom: "concat_stem_1"
  top: "deconv2_2x_c3"
  param {
    name: "fc1_2x_c3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc1_2x_c3_b"
    lr_mult: 2
    decay_mult: 0
  }
 convolution_param {
    num_output: 2
    kernel_size: 4
	stride: 2	
    pad: 34
    dilation: 23
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

### SUM the all branches

layer {
  bottom: "deconv2_2x_d1"
  bottom: "deconv2_2x_c0"
  bottom: "deconv2_2x_c1"
  bottom: "deconv2_2x_c2"
  bottom: "deconv2_2x_c3"
  top: "deconv2_2x"
  name: "fc1_2x"
  type: "Eltwise"
  eltwise_param {
    operation: SUM
  }
}


layer {
  name: "fc1_4x_c0"
  type: "Deconvolution"
  bottom: "conv3_sum"
  top: "deconv3_4x_c0"
  param {
    name: "fc1_4x_c0_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc1_4x_c0_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 2
    kernel_size: 8
	 stride: 4	
    pad: 16
    dilation:5
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
#--------------------4x upsampling & classfier---------------------
  layer{
         name: "deconv3_4x_d1"
           type: "Deconvolution"
           bottom: "conv3_sum"
           top: "deconv3_4x_d1"
           param {lr_mult: 1} param {lr_mult: 2}
            convolution_param {
              num_output: 2 kernel_size: 8 stride: 4 pad:2
              weight_filler {type: "bilinear"}
            }
         }

layer {
  name: "fc1_4x_c1"
  type: "Deconvolution"
  bottom: "conv3_sum"
  top: "deconv3_4x_c1"
  param {
    name: "fc1_4x_c1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc1_4x_c1_b"
    lr_mult: 2
    decay_mult: 0
  }
convolution_param {
    num_output: 2
    kernel_size: 8
	 stride: 4	
    pad: 37
    dilation: 11
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

layer {
  name: "fc1_4x_c2"
   type: "Deconvolution"
  bottom: "conv3_sum"
  top: "deconv3_4x_c2"
  param {
    name: "fc1_4x_c2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc1_4x_c2_b"
    lr_mult: 2
    decay_mult: 0
  }
convolution_param {
    num_output: 2
    kernel_size: 8
	 stride: 4	
    pad: 58
    dilation: 17
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

layer {
  name: "fc1_4x_c3"
   type: "Deconvolution"
  bottom: "conv3_sum"
  top: "deconv3_4x_c3"
  param {
    name: "fc1_4x_c3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc1_4x_c3_b"
    lr_mult: 2
    decay_mult: 0
  }
convolution_param {
    num_output: 2
    kernel_size: 8
	 stride: 4	
    pad: 79
    dilation: 23
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}


layer {
  bottom: "deconv3_4x_d1"
  bottom: "deconv3_4x_c0"
  bottom: "deconv3_4x_c1"
  bottom: "deconv3_4x_c2"
  bottom: "deconv3_4x_c3"
  top: "deconv3_4x"
  name: "fc1_4x"
  type: "Eltwise"
  eltwise_param {
    operation: SUM
  }
}












#------------------------8x upsampling -----------------------
      layer{
         name: "deconv4_8x_d1"
           type: "Deconvolution"
           bottom: "conv4_sum"
           top: "deconv4_8x_d1"
          param {lr_mult: 1} param {lr_mult: 2}
           convolution_param {
             num_output: 2 kernel_size: 16 stride: 8 pad:4
             weight_filler {type: "bilinear"}
           }
         }
layer {
  name: "fc1_8x_c0"
  type: "Deconvolution"
  bottom: "conv4_sum"
  top: "deconv4_8x_c0"
  param {
    name: "fc1_8x_c0_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc1_8x_c0_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 2
    kernel_size: 16
	 stride: 8	
    pad: 34
    dilation: 5
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}


layer {
  name: "fc1_8x_c1"
  type: "Deconvolution"
  bottom: "conv4_sum"
  top: "deconv4_8x_c1"
  param {
    name: "fc1_8x_c1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc1_8x_c1_b"
    lr_mult: 2
    decay_mult: 0
  }
convolution_param {
    num_output: 2
    kernel_size: 16
	 stride: 8	
    pad: 79
    dilation: 11
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

layer {
  name: "fc1_8x_c2"
   type: "Deconvolution"
  bottom: "conv4_sum"
  top: "deconv4_8x_c2"
  param {
    name: "fc1_8x_c2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc1_8x_c2_b"
    lr_mult: 2
    decay_mult: 0
  }
convolution_param {
    num_output: 2
    kernel_size: 16
	 stride: 8	
    pad: 124
    dilation: 17
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}


layer {
  name: "fc1_8x_c3"
   type: "Deconvolution"
  bottom: "conv4_sum"
  top: "deconv4_8x_c3"
  param {
    name: "fc1_8x_c3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc1_8x_c3_b"
    lr_mult: 2
    decay_mult: 0
  }
convolution_param {
    num_output: 2
    kernel_size: 16
	 stride: 8	
    pad: 169
    dilation: 23
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}


layer {
  bottom: "deconv4_8x_d1"
  bottom: "deconv4_8x_c0"
  bottom: "deconv4_8x_c1"
  bottom: "deconv4_8x_c2"
  bottom: "deconv4_8x_c3"
  top: "deconv4_8x"
  name: "fc1_8x"
  type: "Eltwise"
  eltwise_param {
    operation: SUM
  }
}

		 
		 
		 
		 
		 
		 
		 
		 
		 layer {
				bottom: "deconv5_16x" 	top: "deconv5_16x" 	name: "bn_deconv5_16x"	type: "BatchNorm"
			}

		layer {
			bottom: "deconv5_16x" 	top: "deconv5_16x" 	name: "scale_deconv5_16x"	type: "Scale"
			scale_param {	bias_term: true}
		}
		layer { name: "relu_deconv5_16x" type: "ReLU" bottom: "deconv5_16x" top: "deconv5_16x"}
		
	 layer {
				bottom: "deconv4_8x" 	top: "deconv4_8x" 	name: "bn_deconv4_8x"	type: "BatchNorm"
			}

		layer {
			bottom: "deconv4_8x" 	top: "deconv4_8x" 	name: "scale_deconv4_8x"	type: "Scale"
			scale_param {	bias_term: true}
		}
		layer { name: "relu_deconv4_8x" type: "ReLU" bottom: "deconv4_8x" top: "deconv4_8x"}
		
	 layer {
				bottom: "deconv3_4x" 	top: "deconv3_4x" 	name: "bn_deconv3_4x"	type: "BatchNorm"
			}

		layer {
			bottom: "deconv3_4x" 	top: "deconv3_4x" 	name: "scale_deconv3_4x"	type: "Scale"
			scale_param {	bias_term: true}
		}
		layer { name: "relu_deconv3_4x" type: "ReLU" bottom: "deconv3_4x" top: "deconv3_4x"}
		
	 layer {
				bottom: "deconv2_2x" 	top: "deconv2_2x" 	name: "bn_deconv2_2x"	type: "BatchNorm"
			}

		layer {
			bottom: "deconv2_2x" 	top: "deconv2_2x" 	name: "scale_deconv2_2x"	type: "Scale"
			scale_param {	bias_term: true}
		}
		layer { name: "relu_deconv2_2x" type: "ReLU" bottom: "deconv2_2x" top: "deconv2_2x"}
		 
		  layer {
           name: "conv_deconv5_16x"
           type: "Convolution"
           bottom: "deconv5_16x"
           top: "conv_deconv5_16x"
           param {lr_mult: 1} param {lr_mult: 2}
           convolution_param {
             num_output: 2 kernel_size: 3 stride: 1 pad: 1
             weight_filler {type: "gaussian" std: 0.001}
             bias_filler {type: "constant"value: 0}
           }
         }
		 

		 
		 
		 

         layer {
           name: "conv_deconv4_8x"
           type: "Convolution"
           bottom: "deconv4_8x"
           top: "conv_deconv4_8x"
           param {lr_mult: 1} param {lr_mult: 2}
           convolution_param {
             num_output: 2 kernel_size: 3 stride: 1 pad: 1
             weight_filler {type: "gaussian" std: 0.001}
             bias_filler {type: "constant"value: 0}
           }
         }

         layer {
           name: "conv_deconv3_4x"
           type: "Convolution"
           bottom: "deconv3_4x"
           top: "conv_deconv3_4x"
           param {lr_mult: 1} param {lr_mult: 2}
           convolution_param {
             num_output: 2 kernel_size: 3 stride: 1 pad:1
             weight_filler {type: "gaussian" std: 0.001}
             bias_filler {type: "constant"value: 0}
           }
         }

         layer {
           name: "conv_deconv2_2x"
           type: "Convolution"
           bottom: "deconv2_2x"
           top: "conv_deconv2_2x"
           param {lr_mult: 1} param {lr_mult: 2}
           convolution_param {
             num_output: 2 kernel_size: 3 stride: 1 pad:1
             weight_filler {type: "gaussian" std: 0.001}
             bias_filler {type: "constant"value: 0}
           }
      }
	  	 layer {
				bottom:"conv_deconv5_16x" 	top:"conv_deconv5_16x" 	name: "bn_conv_deconv5_16x"	type: "BatchNorm"
			}
		layer {
			bottom:"conv_deconv5_16x" 	top:"conv_deconv5_16x" 	name: "scale_conv_deconv5_16x"	type: "Scale"
			scale_param {	bias_term: true}
		}
		layer { name:"relu_conv_deconv5_16x" type: "ReLU" bottom:"conv_deconv5_16x" top:"conv_deconv5_16x"}
		
		
	 layer {
				bottom:"conv_deconv4_8x" 	top:"conv_deconv4_8x" 	name: "bn_conv_deconv4_8x"	type: "BatchNorm"
			}    
		layer {
			bottom:"conv_deconv4_8x" 	top:"conv_deconv4_8x" 	name: "scale_conv_deconv4_8x"	type: "Scale"
			scale_param {	bias_term: true}
		}
		layer { name:"relu_conv_deconv4_8x" type: "ReLU" bottom:"conv_deconv4_8x" top:"conv_deconv4_8x"}
		
	 
	 layer {
				bottom:"conv_deconv3_4x" 	top:"conv_deconv3_4x" 	name: "bn_conv_deconv3_4x"	type: "BatchNorm"
			}
		layer {
			bottom:"conv_deconv3_4x" 	top:"conv_deconv3_4x" 	name: "scale_conv_deconv3_4x"	type: "Scale"
			scale_param {	bias_term: true}
		}
     layer { name:"relu_conv_deconv3_4x" type: "ReLU" bottom:"conv_deconv3_4x" top:"conv_deconv3_4x"}		
	 
	 layer {
				bottom:"conv_deconv2_2x" 	top:"conv_deconv2_2x" 	name: "bn_conv_deconv2_2x"	type: "BatchNorm"
			}
		layer {
			bottom:"conv_deconv2_2x" 	top:"conv_deconv2_2x" 	name: "scale_conv_deconv2_2x"	type: "Scale"
			scale_param {	bias_term: true}
		}
		layer { name:"relu_conv_deconv2_2x" type: "ReLU" bottom:"conv_deconv2_2x" top:"conv_deconv2_2x"}

# layer {
  # name: "relu1_3b" type: "ReLU" bottom: "deconv1_3b" top: "deconv1_3b" }



         
         layer{
          name: "deconv_all_sum"
          type:  "Eltwise"
          bottom: "conv_deconv2_2x"
          bottom: "conv_deconv3_4x"
          bottom: "conv_deconv4_8x"
		  bottom: "conv_deconv5_16x"
          # bottom: "de_deconv4_8x"
          #bottom: "de_deconv3_4x"
          #bottom: "de_deconv2_2x"
          top:"deconv_all_sum"
          eltwise_param{
              operation: SUM
			  coeff: 1
			  coeff: 1
			  coeff: 1
			  coeff: 1
            }
     
          }

#--------lose---------------------acc-----------
# layer {
#    name: "loss_conv2"
#    type: "SoftmaxWithLoss"
#    bottom: "conv_deconv2_2x"
#    bottom: "label"
#    top: "loss_conv2"
#    label_select_param{
#    	balance: true
#    	num_labels: 3
#    	num_top_label_balance: 2
#    	reorder_label: false
#    	class_prob_mapping_file: '../label_class_selection_for_train.prototxt'
#  	}
#     loss_param{ignore_label:2}
#     loss_weight: 1
#  }

 # layer {
 #    name: "loss_conv3"
 #    type: "SoftmaxWithLoss"
 #    bottom: "conv_deconv3_4x"
 #    bottom: "label"
 #    top: "loss_conv3"
 #    label_select_param{
 #    	balance: true
 #    	num_labels: 3
 #    	num_top_label_balance: 2
 #    	reorder_label: false
 #    	class_prob_mapping_file: '../label_class_selection_for_train.prototxt'
 #  	}
 #     loss_param{ignore_label:2}
 #     loss_weight: 0.6
 #  }

  # layer {
  #    name: "loss_conv4"
  #    type: "SoftmaxWithLoss"
  #    bottom: "conv_deconv4_8x"
  #    bottom: "label"
  #    top: "loss_conv4"
  #    label_select_param{
  #    	balance: true
  #    	num_labels: 3
  #    	num_top_label_balance: 2
  #    	reorder_label: false
  #    	class_prob_mapping_file: '../label_class_selection_for_train.prototxt'
  #  	}
  #     loss_param{ignore_label:2}
  #     loss_weight: 0.6
  #  }


  #  layer {
  #     name: "loss_deconv4"
  #     type: "SoftmaxWithLoss"
  #     bottom: "de_deconv4_8x"
  #     bottom: "label"
  #     top: "loss_deconv4"
  #     label_select_param{
  #     	balance: true
  #     	num_labels: 3
  #     	num_top_label_balance: 2
  #     	reorder_label: false
  #     	class_prob_mapping_file: '../label_class_selection_for_train.prototxt'
  #   	}
  #      loss_param{ignore_label:2}
  #      loss_weight: 0.2
  #   }

    # layer {
    #    name: "loss_deconv3"
    #    type: "SoftmaxWithLoss"
    #    bottom: "de_deconv3_4x"
    #    bottom: "label"
    #    top: "loss_deconv3"
    #    label_select_param{
    #      balance: true
    #      num_labels: 3
    #      num_top_label_balance: 2
    #      reorder_label: false
    #      class_prob_mapping_file: '../label_class_selection_for_train.prototxt'
    #    }
    #     loss_param{ignore_label:2}
    #     loss_weight: 0.5
    #  }

    #  layer {
    #     name: "loss_deconv2"
    #     type: "SoftmaxWithLoss"
    #     bottom: "de_deconv2_2x"
    #     bottom: "label"
    #     top: "loss_deconv2"
    #     label_select_param{
    #       balance: true
    #       num_labels: 3
    #       num_top_label_balance: 2
    #       reorder_label: false
    #       class_prob_mapping_file: '../label_class_selection_for_train.prototxt'
    #     }
    #      loss_param{ignore_label:2}
    #      loss_weight: 1
    #   }


      layer {
        name: "loss_deconv_all"
        type: "SoftmaxWithLoss"
        bottom: "deconv_all_sum"
        bottom: "label"
        top: "loss_deconv_all"
        label_select_param{
			auto_balance: true
        	balance: true
        	num_labels: 2
        	num_top_label_balance: 2
        	reorder_label: false
        	class_prob_mapping_file: 'label_class_selection.prototxt'
      	}
         # loss_param{
           # ignore_label:2
         # }
      
      }

# layer {
  # name: "loss_deconv"
  # type: "SoftmaxWithLoss"
  # bottom: "seg_score"
  # bottom: "label"
  # top: "loss_deconv_last"
  # label_select_param{
  	# balance: true
  	# num_labels: 2
  	# num_top_label_balance: 2
  	# reorder_label: false
  	# class_prob_mapping_file: '../label_class_selection_for_train_ub.prototxt'
	# }
   #loss_param{
   #  ignore_label:2
   #}
   #loss_weight: 1

# }

# layer{
     # name: "accuracy"
     # type: "Accuracy"
     # bottom: "seg_score"
     # bottom: "label"
	 # top: "accuracy"
	#accuracy_param{
	#ignore_label:2
	#}
	 # include {
    # phase: TEST
   # }
# }

layer{
     name: "accuracy_conv"
     type: "Accuracy"
     bottom: "deconv_all_sum"
     bottom: "label"
	 top: "accuracy_conv"
	 top: "class_Acc"
	 accuracy_param{
		ignore_label:2
		}
	 include {
		phase: TEST
	}
}
